{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sklearn as sk\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = sk.datasets.load_diabetes()\n",
    "#both numpy arrays\n",
    "#feature matrix\n",
    "data = diabetes.data\n",
    "#one dimensional numpy array\n",
    "target = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 442\n",
      "Number of features: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of data points: {data.shape[0]}\\nNumber of features: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make train test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X):\n",
    "    #create column vector of ones\n",
    "    weights = np.zeros((X_train.shape[1], 1))\n",
    "    bias = 0.0\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(X, weights, bias):\n",
    "    #linear regression is as follows, y = Xw + b\n",
    "    y = X @ weights + bias\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checking y dimensions, should be (353, 1): (353, 1)\n",
      "Y prediction vector: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "w, b = initialize_parameters(X_train)\n",
    "y_pred = compute_predictions(X_train, w, b)\n",
    "\n",
    "print(f\"Sanity checking y dimensions, should be ({X_train.shape[0]}, 1): {y_pred.shape}\")\n",
    "print(f\"Y prediction vector: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, y_true):\n",
    "    #our loss function is the Mean Squared error\n",
    "    n = y_true.shape[0]\n",
    "    loss = (1 / n) * np.sum((y_pred - y_true) ** 2)\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_w_wrt_loss(y_pred, y_true, X):\n",
    "    '''\n",
    "    The derivative of the loss function with respect to the weights\n",
    "    L = 1/n * SIGMA((y_pred - y_true)**2)\n",
    "    Plug in y_pred = x_i*w + b and y_true = y_i\n",
    "    L = 1/n * SIGMA(x_i*w + b - y_i)**2\n",
    "    dL/dw = 1/n * SIGMA(2*x_i*(x_i*w + b - y_i))\n",
    "    dL/dw = 2/n * SIGMA((x_i*w + b - y_i) *x_i) #move the 2 out of the sum\n",
    "    '''\n",
    "    n = y_pred.shape[0]\n",
    "    error = y_pred - y_true\n",
    "    #X.T has shape (10, 353) and error has shape (353, 1) and dw should have shape (10, 1)\n",
    "    gradient = 2/n * np.dot(X.T, (y_pred - y_true))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_b_wrt_loss(y_pred, y_true,X):\n",
    "    '''\n",
    "    The derivative of the loss function with respect to the bias\n",
    "    L = 1/n * SIGMA((y_pred - y_true)**2)\n",
    "    Plug in y_pred = x_i*w + b and y_true = y_i\n",
    "    L = 1/n * SIGMA(x_i*w + b - y_i)**2\n",
    "    dL/dw = 1/n * SIGMA(2(x_i*w + b - y_i))\n",
    "    dL/dw = 2/n * SIGMA((x_i*w + b - y_i)) #move the 2 out of the sum\n",
    "    '''\n",
    "    n = y_pred.shape[0]\n",
    "    error = y_pred - y_true\n",
    "    gradient = 2/n * (np.sum(error)) #take the sum since b is a scalar\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_via_gradient_descent(X_train, y_train, w, b, learning_rate=0.01, num_iterations=1000):\n",
    "    y_pred = compute_predictions(X_train, w, b)\n",
    "    y_prev = y_pred\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"Iteration {i+1} loss: {compute_loss(y_pred, y_train)}\")\n",
    "        dw = derivative_w_wrt_loss(y_pred, y_train, X_train)\n",
    "        db = derivative_b_wrt_loss(y_pred, y_train, X_train)\n",
    "        w = w - learning_rate * dw\n",
    "        b =  b - learning_rate *db\n",
    "        y_pred = compute_predictions(X_train, w, b)\n",
    "\n",
    "    return w, b, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 loss: 10488097.0\n",
      "Iteration 2 loss: 55548499.72330002\n",
      "Iteration 3 loss: 343975631.5148714\n",
      "Iteration 4 loss: 2190168859.3995433\n",
      "Iteration 5 loss: 14007467091.766544\n",
      "Iteration 6 loss: 89648811347.32436\n",
      "Iteration 7 loss: 573821491792.7247\n",
      "Iteration 8 loss: 3672962402055.6816\n",
      "Iteration 9 loss: 23510253454557.844\n",
      "Iteration 10 loss: 150486769752518.84\n",
      "Iteration 11 loss: 963250752924138.2\n",
      "Iteration 12 loss: 6165671732807364.0\n",
      "Iteration 13 loss: 3.946584818294191e+16\n",
      "Iteration 14 loss: 2.5261694762260822e+17\n",
      "Iteration 15 loss: 1.6169758200259674e+18\n",
      "Iteration 16 loss: 1.0350100526392609e+19\n",
      "Iteration 17 loss: 6.624995845937491e+19\n",
      "Iteration 18 loss: 4.24059359102602e+20\n",
      "Iteration 19 loss: 2.714361551679834e+21\n",
      "Iteration 20 loss: 1.737435685614744e+22\n",
      "Iteration 21 loss: 1.1121152080051409e+23\n",
      "Iteration 22 loss: 7.118538234920117e+23\n",
      "Iteration 23 loss: 4.5565051387900187e+24\n",
      "Iteration 24 loss: 2.916573374288103e+25\n",
      "Iteration 25 loss: 1.8668694511480744e+26\n",
      "Iteration 26 loss: 1.1949644669853712e+27\n",
      "Iteration 27 loss: 7.648848056726668e+27\n",
      "Iteration 28 loss: 4.895951152630168e+28\n",
      "Iteration 29 loss: 3.1338493732870494e+29\n",
      "Iteration 30 loss: 2.005945645347307e+30\n",
      "Iteration 31 loss: 1.2839857481303603e+31\n",
      "Iteration 32 loss: 8.218664375207622e+31\n",
      "Iteration 33 loss: 5.260684879926639e+32\n",
      "Iteration 34 loss: 3.367311784792243e+33\n",
      "Iteration 35 loss: 2.155382600327668e+34\n",
      "Iteration 36 loss: 1.3796388486437371e+35\n",
      "Iteration 37 loss: 8.830930306283681e+35\n",
      "Iteration 38 loss: 5.652590179749121e+36\n",
      "Iteration 39 loss: 3.6181664481556166e+37\n",
      "Iteration 40 loss: 2.3159521617999288e+38\n",
      "Iteration 41 loss: 1.4824178192465153e+39\n",
      "Iteration 42 loss: 9.488808219215031e+39\n",
      "Iteration 43 loss: 6.073691253037349e+40\n",
      "Iteration 44 loss: 3.88770903415668e+41\n",
      "Iteration 45 loss: 2.4884836756733444e+42\n",
      "Iteration 46 loss: 1.592853515961751e+43\n",
      "Iteration 47 loss: 1.0195696070319573e+44\n",
      "Iteration 48 loss: 6.5261630976508704e+44\n",
      "Iteration 49 loss: 4.177331737175341e+45\n",
      "Iteration 50 loss: 2.6738682716485624e+46\n",
      "Iteration 51 loss: 1.7115163419995297e+47\n",
      "Iteration 52 loss: 1.0955244953504783e+48\n",
      "Iteration 53 loss: 7.012342742288883e+48\n",
      "Iteration 54 loss: 4.4885304659116983e+49\n",
      "Iteration 55 loss: 2.8730634659254246e+50\n",
      "Iteration 56 loss: 1.8390191939042027e+51\n",
      "Iteration 57 loss: 1.1771377958261424e+52\n",
      "Iteration 58 loss: 7.534741317303549e+52\n",
      "Iteration 59 loss: 4.822912569792829e+53\n",
      "Iteration 60 loss: 3.087098106798693e+54\n",
      "Iteration 61 loss: 1.9760206271807756e+55\n",
      "Iteration 62 loss: 1.2648310432521446e+56\n",
      "Iteration 63 loss: 8.09605702475267e+56\n",
      "Iteration 64 loss: 5.182205140973932e+57\n",
      "Iteration 65 loss: 3.317077688686002e+58\n",
      "Iteration 66 loss: 2.123228257751022e+59\n",
      "Iteration 67 loss: 1.3590571755038506e+60\n",
      "Iteration 68 loss: 8.699189074682599e+60\n",
      "Iteration 69 loss: 5.568263934813583e+61\n",
      "Iteration 70 loss: 3.5641900620348266e+62\n",
      "Iteration 71 loss: 2.281402416807871e+63\n",
      "Iteration 72 loss: 1.4603028729745535e+64\n",
      "Iteration 73 loss: 9.347252659622819e+64\n",
      "Iteration 74 loss: 5.983082954897965e+65\n",
      "Iteration 75 loss: 3.8297115686006425e+66\n",
      "Iteration 76 loss: 2.451360077945584e+67\n",
      "Iteration 77 loss: 1.5690910722921915e+68\n",
      "Iteration 78 loss: 1.0043595044635097e+69\n",
      "Iteration 79 loss: 6.4288047521204716e+69\n",
      "Iteration 80 loss: 4.115013633784797e+70\n",
      "Iteration 81 loss: 2.6339790768493113e+71\n",
      "Iteration 82 loss: 1.6859836673004792e+72\n",
      "Iteration 83 loss: 1.0791812856023653e+73\n",
      "Iteration 84 loss: 6.90773149101218e+73\n",
      "Iteration 85 loss: 4.4215698500819845e+74\n",
      "Iteration 86 loss: 2.8302026453389742e+75\n",
      "Iteration 87 loss: 1.811584411255027e+76\n",
      "Iteration 88 loss: 1.1595770658002323e+77\n",
      "Iteration 89 loss: 7.422336840480693e+77\n",
      "Iteration 90 loss: 4.750963588223288e+78\n",
      "Iteration 91 loss: 3.041044283185849e+79\n",
      "Iteration 92 loss: 1.94654203522443e+80\n",
      "Iteration 93 loss: 1.2459620913268046e+81\n",
      "Iteration 94 loss: 7.975278750373752e+81\n",
      "Iteration 95 loss: 5.104896175326739e+82\n",
      "Iteration 96 loss: 3.2675929928649015e+83\n",
      "Iteration 97 loss: 2.0915535988028896e+84\n",
      "Iteration 98 loss: 1.338782543057744e+85\n",
      "Iteration 99 loss: 8.569413179858313e+85\n",
      "Iteration 100 loss: 5.485195682295507e+86\n",
      "Iteration 101 loss: 3.5110189042805385e+87\n",
      "Iteration 102 loss: 2.2473680904409314e+88\n",
      "Iteration 103 loss: 1.4385178410103342e+89\n",
      "Iteration 104 loss: 9.207808848523076e+89\n",
      "Iteration 105 loss: 5.8938263658511425e+90\n",
      "Iteration 106 loss: 3.772579318517661e+91\n",
      "Iteration 107 loss: 2.4147902959899647e+92\n",
      "Iteration 108 loss: 1.5456831205602164e+93\n",
      "Iteration 109 loss: 9.893763086393899e+93\n",
      "Iteration 110 loss: 6.332898813969873e+94\n",
      "Iteration 111 loss: 4.0536252018339736e+95\n",
      "Iteration 112 loss: 2.5946849554419076e+96\n",
      "Iteration 113 loss: 1.6608318931288093e+97\n",
      "Iteration 114 loss: 1.063081886472821e+98\n",
      "Iteration 115 loss: 6.804680847123883e+98\n",
      "Iteration 116 loss: 4.3556081634355304e+99\n",
      "Iteration 117 loss: 2.7879812293334493e+100\n",
      "Iteration 118 loss: 1.784558905084049e+101\n",
      "Iteration 119 loss: 1.1422783095552467e+102\n",
      "Iteration 120 loss: 7.311609231632181e+102\n",
      "Iteration 121 loss: 4.680087953075451e+103\n",
      "Iteration 122 loss: 2.995677497884059e+104\n",
      "Iteration 123 loss: 1.9175032096206083e+105\n",
      "Iteration 124 loss: 1.2273746294460535e+106\n",
      "Iteration 125 loss: 7.85630226562126e+106\n",
      "Iteration 126 loss: 5.0287405172015095e+107\n",
      "Iteration 127 loss: 3.218846517655518e+108\n",
      "Iteration 128 loss: 2.060351467486124e+109\n",
      "Iteration 129 loss: 1.3188103708231946e+110\n",
      "Iteration 130 loss: 8.441573302602184e+110\n",
      "Iteration 131 loss: 5.403366655262632e+111\n",
      "Iteration 132 loss: 3.4586409623670587e+112\n",
      "Iteration 133 loss: 2.213841493601533e+113\n",
      "Iteration 134 loss: 1.4170578016394048e+114\n",
      "Iteration 135 loss: 9.070445282513669e+114\n",
      "Iteration 136 loss: 5.805901320884176e+115\n",
      "Iteration 137 loss: 3.7162993764847494e+116\n",
      "Iteration 138 loss: 2.378766067894123e+117\n",
      "Iteration 139 loss: 1.5226243723983491e+118\n",
      "Iteration 140 loss: 9.746166345284615e+118\n",
      "Iteration 141 loss: 6.238423615953231e+119\n",
      "Iteration 142 loss: 3.993152572335498e+120\n",
      "Iteration 143 loss: 2.5559770300262285e+121\n",
      "Iteration 144 loss: 1.6360553371494894e+122\n",
      "Iteration 145 loss: 1.047222660756017e+123\n",
      "Iteration 146 loss: 6.703167529233192e+123\n",
      "Iteration 147 loss: 4.290630503786877e+124\n",
      "Iteration 148 loss: 2.7463896791689467e+125\n",
      "Iteration 149 loss: 1.7579365697392526e+126\n",
      "Iteration 150 loss: 1.125237618924399e+127\n",
      "Iteration 151 loss: 7.20253347497318e+127\n",
      "Iteration 152 loss: 4.6102696519955835e+128\n",
      "Iteration 153 loss: 2.950987501545857e+129\n",
      "Iteration 154 loss: 1.8888975898644844e+130\n",
      "Iteration 155 loss: 1.2090644582963584e+131\n",
      "Iteration 156 loss: 7.739100691109168e+131\n",
      "Iteration 157 loss: 4.953720961372058e+132\n",
      "Iteration 158 loss: 3.1708272501646417e+133\n",
      "Iteration 159 loss: 2.02961481455789e+134\n",
      "Iteration 160 loss: 1.2991361466503616e+135\n",
      "Iteration 161 loss: 8.315640561094301e+135\n",
      "Iteration 162 loss: 5.32275836675086e+136\n",
      "Iteration 163 loss: 3.4070444029735532e+137\n",
      "Iteration 164 loss: 2.1808150518993414e+138\n",
      "Iteration 165 loss: 1.395917906570252e+139\n",
      "Iteration 166 loss: 8.93513092816553e+139\n",
      "Iteration 167 loss: 5.71928795580947e+140\n",
      "Iteration 168 loss: 3.660859027634089e+141\n",
      "Iteration 169 loss: 2.3432792549983027e+142\n",
      "Iteration 170 loss: 1.4999096183318677e+143\n",
      "Iteration 171 loss: 9.600771475980467e+143\n",
      "Iteration 172 loss: 6.1453578140603294e+144\n",
      "Iteration 173 loss: 3.9335820832018767e+145\n",
      "Iteration 174 loss: 2.51784655563669e+146\n",
      "Iteration 175 loss: 1.6116484017974893e+147\n",
      "Iteration 176 loss: 1.0316000255065554e+148\n",
      "Iteration 177 loss: 6.603168603264914e+148\n",
      "Iteration 178 loss: 4.226622191263831e+149\n",
      "Iteration 179 loss: 2.7054185984060674e+150\n",
      "Iteration 180 loss: 1.731711390653742e+151\n",
      "Iteration 181 loss: 1.1084511440435558e+152\n",
      "Iteration 182 loss: 7.09508492790839e+152\n",
      "Iteration 183 loss: 4.541492911504883e+153\n",
      "Iteration 184 loss: 2.906964197725163e+154\n",
      "Iteration 185 loss: 1.8607187133219006e+155\n",
      "Iteration 186 loss: 1.1910274412102155e+156\n",
      "Iteration 187 loss: 7.623647548442469e+156\n",
      "Iteration 188 loss: 4.8798205592825316e+157\n",
      "Iteration 189 loss: 3.1235243417911543e+158\n",
      "Iteration 190 loss: 1.9993366959371033e+159\n",
      "Iteration 191 loss: 1.2797554257023821e+160\n",
      "Iteration 192 loss: 8.191586504378389e+160\n",
      "Iteration 193 loss: 5.243352605587558e+161\n",
      "Iteration 194 loss: 3.3562175693105473e+162\n",
      "Iteration 195 loss: 2.148281303939985e+163\n",
      "Iteration 196 loss: 1.3750933798389463e+164\n",
      "Iteration 197 loss: 8.801835215011108e+164\n",
      "Iteration 198 loss: 5.633966702776467e+165\n",
      "Iteration 199 loss: 3.606245746780184e+166\n",
      "Iteration 200 loss: 2.3083218400565282e+167\n",
      "Iteration 201 loss: 1.477533726601781e+168\n",
      "Iteration 202 loss: 9.457545630605346e+168\n",
      "Iteration 203 loss: 6.053680382694187e+169\n",
      "Iteration 204 loss: 3.8749002761587127e+170\n",
      "Iteration 205 loss: 2.480284917766429e+171\n",
      "Iteration 206 loss: 1.5876055730131154e+172\n",
      "Iteration 207 loss: 1.0162104512299666e+173\n",
      "Iteration 208 loss: 6.504661477277883e+173\n",
      "Iteration 209 loss: 4.1635687649908e+174\n",
      "Iteration 210 loss: 2.6650587307829675e+175\n",
      "Iteration 211 loss: 1.7058774429868673e+176\n",
      "Iteration 212 loss: 1.0919150924814644e+177\n",
      "Iteration 213 loss: 6.98923931546461e+177\n",
      "Iteration 214 loss: 4.4737421934357514e+178\n",
      "Iteration 215 loss: 2.863597640596287e+179\n",
      "Iteration 216 loss: 1.8329602137692743e+180\n",
      "Iteration 217 loss: 1.1732595032315744e+181\n",
      "Iteration 218 loss: 7.509916754234995e+181\n",
      "Iteration 219 loss: 4.807022615218271e+182\n",
      "Iteration 220 loss: 3.076927105775063e+183\n",
      "Iteration 221 loss: 1.969510271135559e+184\n",
      "Iteration 222 loss: 1.2606638294511595e+185\n",
      "Iteration 223 loss: 8.069383105933914e+185\n",
      "Iteration 224 loss: 5.16513143227724e+186\n",
      "Iteration 225 loss: 3.306148978486337e+187\n",
      "Iteration 226 loss: 2.1162328996393176e+188\n",
      "Iteration 227 loss: 1.35457951673013e+189\n",
      "Iteration 228 loss: 8.670528028637875e+189\n",
      "Iteration 229 loss: 5.549918285850819e+190\n",
      "Iteration 230 loss: 3.552447195590255e+191\n",
      "Iteration 231 loss: 2.2738859254253633e+192\n",
      "Iteration 232 loss: 1.4554916420055215e+193\n",
      "Iteration 233 loss: 9.316456451313145e+193\n",
      "Iteration 234 loss: 5.963370609921038e+194\n",
      "Iteration 235 loss: 3.817093893704356e+195\n",
      "Iteration 236 loss: 2.4432836304212256e+196\n",
      "Iteration 237 loss: 1.5639214189963193e+197\n",
      "Iteration 238 loss: 1.0010504610853557e+198\n",
      "Iteration 239 loss: 6.407623896361252e+198\n",
      "Iteration 240 loss: 4.101455979821876e+199\n",
      "Iteration 241 loss: 2.625300958124187e+200\n",
      "Iteration 242 loss: 1.680428890285709e+201\n",
      "Iteration 243 loss: 1.075625728382979e+202\n",
      "Iteration 244 loss: 6.884972724806611e+202\n",
      "Iteration 245 loss: 4.4070021914214645e+203\n",
      "Iteration 246 loss: 2.820878032706959e+204\n",
      "Iteration 247 loss: 1.8056158199553967e+205\n",
      "Iteration 248 loss: 1.1557566301952482e+206\n",
      "Iteration 249 loss: 7.397882614216761e+206\n",
      "Iteration 250 loss: 4.7353106825340065e+207\n",
      "Iteration 251 loss: 3.0310250147831996e+208\n",
      "Iteration 252 loss: 1.9401288017125808e+209\n",
      "Iteration 253 loss: 1.2418570446882068e+210\n",
      "Iteration 254 loss: 7.94900275734473e+210\n",
      "Iteration 255 loss: 5.088077174948791e+211\n",
      "Iteration 256 loss: 3.2568273189129722e+212\n",
      "Iteration 257 loss: 2.084662598563007e+213\n",
      "Iteration 258 loss: 1.334371682714197e+214\n",
      "Iteration 259 loss: 8.541179703885303e+214\n",
      "Iteration 260 loss: 5.46712371665994e+215\n",
      "Iteration 261 loss: 3.4994512197968616e+216\n",
      "Iteration 262 loss: 2.239963731279777e+217\n",
      "Iteration 263 loss: 1.4337783847548679e+218\n",
      "Iteration 264 loss: 9.177472062977434e+218\n",
      "Iteration 265 loss: 5.874408092791224e+219\n",
      "Iteration 266 loss: 3.7601498761147314e+220\n",
      "Iteration 267 loss: 2.4068343342022843e+221\n",
      "Iteration 268 loss: 1.5405905889795397e+222\n",
      "Iteration 269 loss: 9.861166300999146e+222\n",
      "Iteration 270 loss: 6.31203393760654e+223\n",
      "Iteration 271 loss: 4.040269803122569e+224\n",
      "Iteration 272 loss: 2.5861362982807243e+225\n",
      "Iteration 273 loss: 1.6553599831665124e+226\n",
      "Iteration 274 loss: 1.0595793716250542e+227\n",
      "Iteration 275 loss: 6.782261599834818e+227\n",
      "Iteration 276 loss: 4.341257827438277e+228\n",
      "Iteration 277 loss: 2.778795722764968e+229\n",
      "Iteration 278 loss: 1.7786793541846324e+230\n",
      "Iteration 279 loss: 1.1385148678200436e+231\n",
      "Iteration 280 loss: 7.287519817429308e+231\n",
      "Iteration 281 loss: 4.6646685599383265e+232\n",
      "Iteration 282 loss: 2.985807698530924e+233\n",
      "Iteration 283 loss: 1.911185649752661e+234\n",
      "Iteration 284 loss: 1.2233308225501791e+235\n",
      "Iteration 285 loss: 7.830418262061439e+235\n",
      "Iteration 286 loss: 5.012172425362914e+236\n",
      "Iteration 287 loss: 3.208241447750546e+237\n",
      "Iteration 288 loss: 2.0535632682906428e+238\n",
      "Iteration 289 loss: 1.314465312400159e+239\n",
      "Iteration 290 loss: 8.413761018142193e+239\n",
      "Iteration 291 loss: 5.385564290102624e+240\n",
      "Iteration 292 loss: 3.4472458464517896e+241\n",
      "Iteration 293 loss: 2.206547593855325e+242\n",
      "Iteration 294 loss: 1.4123890493508535e+243\n",
      "Iteration 295 loss: 9.040561065989874e+243\n",
      "Iteration 296 loss: 5.786772732729462e+244\n",
      "Iteration 297 loss: 3.7040553584928017e+245\n",
      "Iteration 298 loss: 2.370928794417661e+246\n",
      "Iteration 299 loss: 1.5176078120188e+247\n",
      "Iteration 300 loss: 9.714055843951146e+247\n",
      "Iteration 301 loss: 6.217870005154685e+248\n",
      "Iteration 302 loss: 3.979996411599453e+249\n",
      "Iteration 303 loss: 2.5475559031007004e+250\n",
      "Iteration 304 loss: 1.6306650580157298e+251\n",
      "Iteration 305 loss: 1.0437723969852868e+252\n",
      "Iteration 306 loss: 6.681082735863126e+252\n",
      "Iteration 307 loss: 4.276494248398622e+253\n",
      "Iteration 308 loss: 2.73734120345747e+254\n",
      "Iteration 309 loss: 1.7521447309210937e+255\n",
      "Iteration 310 loss: 1.1215303208152828e+256\n",
      "Iteration 311 loss: 7.178803430506552e+256\n",
      "Iteration 312 loss: 4.5950802878329386e+257\n",
      "Iteration 313 loss: 2.941264941438988e+258\n",
      "Iteration 314 loss: 1.882674276365678e+259\n",
      "Iteration 315 loss: 1.2050809775589063e+260\n",
      "Iteration 316 loss: 7.713602829256792e+260\n",
      "Iteration 317 loss: 4.9374000349789865e+261\n",
      "Iteration 318 loss: 3.1603803883897015e+262\n",
      "Iteration 319 loss: 2.0229278828043655e+263\n",
      "Iteration 320 loss: 1.2948559085042468e+264\n",
      "Iteration 321 loss: 8.288243184744839e+264\n",
      "Iteration 322 loss: 5.305221580123319e+265\n",
      "Iteration 323 loss: 3.395819281221133e+266\n",
      "Iteration 324 loss: 2.1736299637168367e+267\n",
      "Iteration 325 loss: 1.3913188034755086e+268\n",
      "Iteration 326 loss: 8.905692529166369e+268\n",
      "Iteration 327 loss: 5.700444730994096e+269\n",
      "Iteration 328 loss: 3.6487976678620135e+270\n",
      "Iteration 329 loss: 2.3355588992218e+271\n",
      "Iteration 330 loss: 1.4949678958028836e+272\n",
      "Iteration 331 loss: 9.569140004244682e+272\n",
      "Iteration 332 loss: 6.125110825316967e+273\n",
      "Iteration 333 loss: 3.920622188177141e+274\n",
      "Iteration 334 loss: 2.509551056430302e+275\n",
      "Iteration 335 loss: 1.6063385357104725e+276\n",
      "Iteration 336 loss: 1.0282012333229167e+277\n",
      "Iteration 337 loss: 6.581413274376677e+277\n",
      "Iteration 338 loss: 4.2126968227957664e+278\n",
      "Iteration 339 loss: 2.6965051093033474e+279\n",
      "Iteration 340 loss: 1.7260059554139787e+280\n",
      "Iteration 341 loss: 1.104799152000934e+281\n",
      "Iteration 342 loss: 7.071708892042775e+281\n",
      "Iteration 343 loss: 4.5265301447076584e+282\n",
      "Iteration 344 loss: 2.8973866803259262e+283\n",
      "Iteration 345 loss: 1.854588240209821e+284\n",
      "Iteration 346 loss: 1.1871033866759032e+285\n",
      "Iteration 347 loss: 7.598530067773799e+285\n",
      "Iteration 348 loss: 4.863743111081328e+286\n",
      "Iteration 349 loss: 3.113233327972051e+287\n",
      "Iteration 350 loss: 1.9927495209016305e+288\n",
      "Iteration 351 loss: 1.2755390408339238e+289\n",
      "Iteration 352 loss: 8.164597846473868e+289\n",
      "Iteration 353 loss: 5.226077435549458e+290\n",
      "Iteration 354 loss: 3.3451599057208556e+291\n",
      "Iteration 355 loss: 2.141203404052858e+292\n",
      "Iteration 356 loss: 1.3705628869001931e+293\n",
      "Iteration 357 loss: 8.772835982759442e+293\n",
      "Iteration 358 loss: 5.615404584204495e+294\n",
      "Iteration 359 loss: 3.5943643203034542e+295\n",
      "Iteration 360 loss: 2.3007166577830423e+296\n",
      "Iteration 361 loss: 1.472665725480347e+297\n",
      "Iteration 362 loss: 9.426386042227162e+297\n",
      "Iteration 363 loss: 6.0337354417691925e+298\n",
      "Iteration 364 loss: 3.862133718922047e+299\n",
      "Iteration 365 loss: 2.4721131721448127e+300\n",
      "Iteration 366 loss: 1.5823749203581758e+301\n",
      "Iteration 367 loss: 1.0128623627720675e+302\n",
      "Iteration 368 loss: 6.483230697867719e+302\n",
      "Iteration 369 loss: 4.1498511373981577e+303\n",
      "Iteration 370 loss: 2.6562782145371886e+304\n",
      "Iteration 371 loss: 1.7002571223431097e+305\n",
      "Iteration 372 loss: inf\n",
      "Iteration 373 loss: inf\n",
      "Iteration 374 loss: inf\n",
      "Iteration 375 loss: inf\n",
      "Iteration 376 loss: inf\n",
      "Iteration 377 loss: inf\n",
      "Iteration 378 loss: inf\n",
      "Iteration 379 loss: inf\n",
      "Iteration 380 loss: inf\n",
      "Iteration 381 loss: inf\n",
      "Iteration 382 loss: inf\n",
      "Iteration 383 loss: inf\n",
      "Iteration 384 loss: inf\n",
      "Iteration 385 loss: inf\n",
      "Iteration 386 loss: inf\n",
      "Iteration 387 loss: inf\n",
      "Iteration 388 loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/_6/c1g55pyd4sb4drdv20_tq3jh0000gn/T/ipykernel_78333/1482752030.py:4: RuntimeWarning: overflow encountered in square\n",
      "  loss = (1 / n) * np.sum((y_pred - y_true) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 389 loss: inf\n",
      "Iteration 390 loss: inf\n",
      "Iteration 391 loss: inf\n",
      "Iteration 392 loss: inf\n",
      "Iteration 393 loss: inf\n",
      "Iteration 394 loss: inf\n",
      "Iteration 395 loss: inf\n",
      "Iteration 396 loss: inf\n",
      "Iteration 397 loss: inf\n",
      "Iteration 398 loss: inf\n",
      "Iteration 399 loss: inf\n",
      "Iteration 400 loss: inf\n",
      "Iteration 401 loss: inf\n",
      "Iteration 402 loss: inf\n",
      "Iteration 403 loss: inf\n",
      "Iteration 404 loss: inf\n",
      "Iteration 405 loss: inf\n",
      "Iteration 406 loss: inf\n",
      "Iteration 407 loss: inf\n",
      "Iteration 408 loss: inf\n",
      "Iteration 409 loss: inf\n",
      "Iteration 410 loss: inf\n",
      "Iteration 411 loss: inf\n",
      "Iteration 412 loss: inf\n",
      "Iteration 413 loss: inf\n",
      "Iteration 414 loss: inf\n",
      "Iteration 415 loss: inf\n",
      "Iteration 416 loss: inf\n",
      "Iteration 417 loss: inf\n",
      "Iteration 418 loss: inf\n",
      "Iteration 419 loss: inf\n",
      "Iteration 420 loss: inf\n",
      "Iteration 421 loss: inf\n",
      "Iteration 422 loss: inf\n",
      "Iteration 423 loss: inf\n",
      "Iteration 424 loss: inf\n",
      "Iteration 425 loss: inf\n",
      "Iteration 426 loss: inf\n",
      "Iteration 427 loss: inf\n",
      "Iteration 428 loss: inf\n",
      "Iteration 429 loss: inf\n",
      "Iteration 430 loss: inf\n",
      "Iteration 431 loss: inf\n",
      "Iteration 432 loss: inf\n",
      "Iteration 433 loss: inf\n",
      "Iteration 434 loss: inf\n",
      "Iteration 435 loss: inf\n",
      "Iteration 436 loss: inf\n",
      "Iteration 437 loss: inf\n",
      "Iteration 438 loss: inf\n",
      "Iteration 439 loss: inf\n",
      "Iteration 440 loss: inf\n",
      "Iteration 441 loss: inf\n",
      "Iteration 442 loss: inf\n",
      "Iteration 443 loss: inf\n",
      "Iteration 444 loss: inf\n",
      "Iteration 445 loss: inf\n",
      "Iteration 446 loss: inf\n",
      "Iteration 447 loss: inf\n",
      "Iteration 448 loss: inf\n",
      "Iteration 449 loss: inf\n",
      "Iteration 450 loss: inf\n",
      "Iteration 451 loss: inf\n",
      "Iteration 452 loss: inf\n",
      "Iteration 453 loss: inf\n",
      "Iteration 454 loss: inf\n",
      "Iteration 455 loss: inf\n",
      "Iteration 456 loss: inf\n",
      "Iteration 457 loss: inf\n",
      "Iteration 458 loss: inf\n",
      "Iteration 459 loss: inf\n",
      "Iteration 460 loss: inf\n",
      "Iteration 461 loss: inf\n",
      "Iteration 462 loss: inf\n",
      "Iteration 463 loss: inf\n",
      "Iteration 464 loss: inf\n",
      "Iteration 465 loss: inf\n",
      "Iteration 466 loss: inf\n",
      "Iteration 467 loss: inf\n",
      "Iteration 468 loss: inf\n",
      "Iteration 469 loss: inf\n",
      "Iteration 470 loss: inf\n",
      "Iteration 471 loss: inf\n",
      "Iteration 472 loss: inf\n",
      "Iteration 473 loss: inf\n",
      "Iteration 474 loss: inf\n",
      "Iteration 475 loss: inf\n",
      "Iteration 476 loss: inf\n",
      "Iteration 477 loss: inf\n",
      "Iteration 478 loss: inf\n",
      "Iteration 479 loss: inf\n",
      "Iteration 480 loss: inf\n",
      "Iteration 481 loss: inf\n",
      "Iteration 482 loss: inf\n",
      "Iteration 483 loss: inf\n",
      "Iteration 484 loss: inf\n",
      "Iteration 485 loss: inf\n",
      "Iteration 486 loss: inf\n",
      "Iteration 487 loss: inf\n",
      "Iteration 488 loss: inf\n",
      "Iteration 489 loss: inf\n",
      "Iteration 490 loss: inf\n",
      "Iteration 491 loss: inf\n",
      "Iteration 492 loss: inf\n",
      "Iteration 493 loss: inf\n",
      "Iteration 494 loss: inf\n",
      "Iteration 495 loss: inf\n",
      "Iteration 496 loss: inf\n",
      "Iteration 497 loss: inf\n",
      "Iteration 498 loss: inf\n",
      "Iteration 499 loss: inf\n",
      "Iteration 500 loss: inf\n",
      "Iteration 501 loss: inf\n",
      "Iteration 502 loss: inf\n",
      "Iteration 503 loss: inf\n",
      "Iteration 504 loss: inf\n",
      "Iteration 505 loss: inf\n",
      "Iteration 506 loss: inf\n",
      "Iteration 507 loss: inf\n",
      "Iteration 508 loss: inf\n",
      "Iteration 509 loss: inf\n",
      "Iteration 510 loss: inf\n",
      "Iteration 511 loss: inf\n",
      "Iteration 512 loss: inf\n",
      "Iteration 513 loss: inf\n",
      "Iteration 514 loss: inf\n",
      "Iteration 515 loss: inf\n",
      "Iteration 516 loss: inf\n",
      "Iteration 517 loss: inf\n",
      "Iteration 518 loss: inf\n",
      "Iteration 519 loss: inf\n",
      "Iteration 520 loss: inf\n",
      "Iteration 521 loss: inf\n",
      "Iteration 522 loss: inf\n",
      "Iteration 523 loss: inf\n",
      "Iteration 524 loss: inf\n",
      "Iteration 525 loss: inf\n",
      "Iteration 526 loss: inf\n",
      "Iteration 527 loss: inf\n",
      "Iteration 528 loss: inf\n",
      "Iteration 529 loss: inf\n",
      "Iteration 530 loss: inf\n",
      "Iteration 531 loss: inf\n",
      "Iteration 532 loss: inf\n",
      "Iteration 533 loss: inf\n",
      "Iteration 534 loss: inf\n",
      "Iteration 535 loss: inf\n",
      "Iteration 536 loss: inf\n",
      "Iteration 537 loss: inf\n",
      "Iteration 538 loss: inf\n",
      "Iteration 539 loss: inf\n",
      "Iteration 540 loss: inf\n",
      "Iteration 541 loss: inf\n",
      "Iteration 542 loss: inf\n",
      "Iteration 543 loss: inf\n",
      "Iteration 544 loss: inf\n",
      "Iteration 545 loss: inf\n",
      "Iteration 546 loss: inf\n",
      "Iteration 547 loss: inf\n",
      "Iteration 548 loss: inf\n",
      "Iteration 549 loss: inf\n",
      "Iteration 550 loss: inf\n",
      "Iteration 551 loss: inf\n",
      "Iteration 552 loss: inf\n",
      "Iteration 553 loss: inf\n",
      "Iteration 554 loss: inf\n",
      "Iteration 555 loss: inf\n",
      "Iteration 556 loss: inf\n",
      "Iteration 557 loss: inf\n",
      "Iteration 558 loss: inf\n",
      "Iteration 559 loss: inf\n",
      "Iteration 560 loss: inf\n",
      "Iteration 561 loss: inf\n",
      "Iteration 562 loss: inf\n",
      "Iteration 563 loss: inf\n",
      "Iteration 564 loss: inf\n",
      "Iteration 565 loss: inf\n",
      "Iteration 566 loss: inf\n",
      "Iteration 567 loss: inf\n",
      "Iteration 568 loss: inf\n",
      "Iteration 569 loss: inf\n",
      "Iteration 570 loss: inf\n",
      "Iteration 571 loss: inf\n",
      "Iteration 572 loss: inf\n",
      "Iteration 573 loss: inf\n",
      "Iteration 574 loss: inf\n",
      "Iteration 575 loss: inf\n",
      "Iteration 576 loss: inf\n",
      "Iteration 577 loss: inf\n",
      "Iteration 578 loss: inf\n",
      "Iteration 579 loss: inf\n",
      "Iteration 580 loss: inf\n",
      "Iteration 581 loss: inf\n",
      "Iteration 582 loss: inf\n",
      "Iteration 583 loss: inf\n",
      "Iteration 584 loss: inf\n",
      "Iteration 585 loss: inf\n",
      "Iteration 586 loss: inf\n",
      "Iteration 587 loss: inf\n",
      "Iteration 588 loss: inf\n",
      "Iteration 589 loss: inf\n",
      "Iteration 590 loss: inf\n",
      "Iteration 591 loss: inf\n",
      "Iteration 592 loss: inf\n",
      "Iteration 593 loss: inf\n",
      "Iteration 594 loss: inf\n",
      "Iteration 595 loss: inf\n",
      "Iteration 596 loss: inf\n",
      "Iteration 597 loss: inf\n",
      "Iteration 598 loss: inf\n",
      "Iteration 599 loss: inf\n",
      "Iteration 600 loss: inf\n",
      "Iteration 601 loss: inf\n",
      "Iteration 602 loss: inf\n",
      "Iteration 603 loss: inf\n",
      "Iteration 604 loss: inf\n",
      "Iteration 605 loss: inf\n",
      "Iteration 606 loss: inf\n",
      "Iteration 607 loss: inf\n",
      "Iteration 608 loss: inf\n",
      "Iteration 609 loss: inf\n",
      "Iteration 610 loss: inf\n",
      "Iteration 611 loss: inf\n",
      "Iteration 612 loss: inf\n",
      "Iteration 613 loss: inf\n",
      "Iteration 614 loss: inf\n",
      "Iteration 615 loss: inf\n",
      "Iteration 616 loss: inf\n",
      "Iteration 617 loss: inf\n",
      "Iteration 618 loss: inf\n",
      "Iteration 619 loss: inf\n",
      "Iteration 620 loss: inf\n",
      "Iteration 621 loss: inf\n",
      "Iteration 622 loss: inf\n",
      "Iteration 623 loss: inf\n",
      "Iteration 624 loss: inf\n",
      "Iteration 625 loss: inf\n",
      "Iteration 626 loss: inf\n",
      "Iteration 627 loss: inf\n",
      "Iteration 628 loss: inf\n",
      "Iteration 629 loss: inf\n",
      "Iteration 630 loss: inf\n",
      "Iteration 631 loss: inf\n",
      "Iteration 632 loss: inf\n",
      "Iteration 633 loss: inf\n",
      "Iteration 634 loss: inf\n",
      "Iteration 635 loss: inf\n",
      "Iteration 636 loss: inf\n",
      "Iteration 637 loss: inf\n",
      "Iteration 638 loss: inf\n",
      "Iteration 639 loss: inf\n",
      "Iteration 640 loss: inf\n",
      "Iteration 641 loss: inf\n",
      "Iteration 642 loss: inf\n",
      "Iteration 643 loss: inf\n",
      "Iteration 644 loss: inf\n",
      "Iteration 645 loss: inf\n",
      "Iteration 646 loss: inf\n",
      "Iteration 647 loss: inf\n",
      "Iteration 648 loss: inf\n",
      "Iteration 649 loss: inf\n",
      "Iteration 650 loss: inf\n",
      "Iteration 651 loss: inf\n",
      "Iteration 652 loss: inf\n",
      "Iteration 653 loss: inf\n",
      "Iteration 654 loss: inf\n",
      "Iteration 655 loss: inf\n",
      "Iteration 656 loss: inf\n",
      "Iteration 657 loss: inf\n",
      "Iteration 658 loss: inf\n",
      "Iteration 659 loss: inf\n",
      "Iteration 660 loss: inf\n",
      "Iteration 661 loss: inf\n",
      "Iteration 662 loss: inf\n",
      "Iteration 663 loss: inf\n",
      "Iteration 664 loss: inf\n",
      "Iteration 665 loss: inf\n",
      "Iteration 666 loss: inf\n",
      "Iteration 667 loss: inf\n",
      "Iteration 668 loss: inf\n",
      "Iteration 669 loss: inf\n",
      "Iteration 670 loss: inf\n",
      "Iteration 671 loss: inf\n",
      "Iteration 672 loss: inf\n",
      "Iteration 673 loss: inf\n",
      "Iteration 674 loss: inf\n",
      "Iteration 675 loss: inf\n",
      "Iteration 676 loss: inf\n",
      "Iteration 677 loss: inf\n",
      "Iteration 678 loss: inf\n",
      "Iteration 679 loss: inf\n",
      "Iteration 680 loss: inf\n",
      "Iteration 681 loss: inf\n",
      "Iteration 682 loss: inf\n",
      "Iteration 683 loss: inf\n",
      "Iteration 684 loss: inf\n",
      "Iteration 685 loss: inf\n",
      "Iteration 686 loss: inf\n",
      "Iteration 687 loss: inf\n",
      "Iteration 688 loss: inf\n",
      "Iteration 689 loss: inf\n",
      "Iteration 690 loss: inf\n",
      "Iteration 691 loss: inf\n",
      "Iteration 692 loss: inf\n",
      "Iteration 693 loss: inf\n",
      "Iteration 694 loss: inf\n",
      "Iteration 695 loss: inf\n",
      "Iteration 696 loss: inf\n",
      "Iteration 697 loss: inf\n",
      "Iteration 698 loss: inf\n",
      "Iteration 699 loss: inf\n",
      "Iteration 700 loss: inf\n",
      "Iteration 701 loss: inf\n",
      "Iteration 702 loss: inf\n",
      "Iteration 703 loss: inf\n",
      "Iteration 704 loss: inf\n",
      "Iteration 705 loss: inf\n",
      "Iteration 706 loss: inf\n",
      "Iteration 707 loss: inf\n",
      "Iteration 708 loss: inf\n",
      "Iteration 709 loss: inf\n",
      "Iteration 710 loss: inf\n",
      "Iteration 711 loss: inf\n",
      "Iteration 712 loss: inf\n",
      "Iteration 713 loss: inf\n",
      "Iteration 714 loss: inf\n",
      "Iteration 715 loss: inf\n",
      "Iteration 716 loss: inf\n",
      "Iteration 717 loss: inf\n",
      "Iteration 718 loss: inf\n",
      "Iteration 719 loss: inf\n",
      "Iteration 720 loss: inf\n",
      "Iteration 721 loss: inf\n",
      "Iteration 722 loss: inf\n",
      "Iteration 723 loss: inf\n",
      "Iteration 724 loss: inf\n",
      "Iteration 725 loss: inf\n",
      "Iteration 726 loss: inf\n",
      "Iteration 727 loss: inf\n",
      "Iteration 728 loss: inf\n",
      "Iteration 729 loss: inf\n",
      "Iteration 730 loss: inf\n",
      "Iteration 731 loss: inf\n",
      "Iteration 732 loss: inf\n",
      "Iteration 733 loss: inf\n",
      "Iteration 734 loss: inf\n",
      "Iteration 735 loss: inf\n",
      "Iteration 736 loss: inf\n",
      "Iteration 737 loss: inf\n",
      "Iteration 738 loss: inf\n",
      "Iteration 739 loss: inf\n",
      "Iteration 740 loss: inf\n",
      "Iteration 741 loss: inf\n",
      "Iteration 742 loss: inf\n",
      "Iteration 743 loss: inf\n",
      "Iteration 744 loss: inf\n",
      "Iteration 745 loss: inf\n",
      "Iteration 746 loss: inf\n",
      "Iteration 747 loss: inf\n",
      "Iteration 748 loss: inf\n",
      "Iteration 749 loss: inf\n",
      "Iteration 750 loss: nan\n",
      "Iteration 751 loss: nan\n",
      "Iteration 752 loss: nan\n",
      "Iteration 753 loss: nan\n",
      "Iteration 754 loss: nan\n",
      "Iteration 755 loss: nan\n",
      "Iteration 756 loss: nan\n",
      "Iteration 757 loss: nan\n",
      "Iteration 758 loss: nan\n",
      "Iteration 759 loss: nan\n",
      "Iteration 760 loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/c1g55pyd4sb4drdv20_tq3jh0000gn/T/ipykernel_78333/1450195062.py:13: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = 2/n *(X.T @ error)\n",
      "/var/folders/_6/c1g55pyd4sb4drdv20_tq3jh0000gn/T/ipykernel_78333/322451642.py:9: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  b =  b - learning_rate *db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 761 loss: nan\n",
      "Iteration 762 loss: nan\n",
      "Iteration 763 loss: nan\n",
      "Iteration 764 loss: nan\n",
      "Iteration 765 loss: nan\n",
      "Iteration 766 loss: nan\n",
      "Iteration 767 loss: nan\n",
      "Iteration 768 loss: nan\n",
      "Iteration 769 loss: nan\n",
      "Iteration 770 loss: nan\n",
      "Iteration 771 loss: nan\n",
      "Iteration 772 loss: nan\n",
      "Iteration 773 loss: nan\n",
      "Iteration 774 loss: nan\n",
      "Iteration 775 loss: nan\n",
      "Iteration 776 loss: nan\n",
      "Iteration 777 loss: nan\n",
      "Iteration 778 loss: nan\n",
      "Iteration 779 loss: nan\n",
      "Iteration 780 loss: nan\n",
      "Iteration 781 loss: nan\n",
      "Iteration 782 loss: nan\n",
      "Iteration 783 loss: nan\n",
      "Iteration 784 loss: nan\n",
      "Iteration 785 loss: nan\n",
      "Iteration 786 loss: nan\n",
      "Iteration 787 loss: nan\n",
      "Iteration 788 loss: nan\n",
      "Iteration 789 loss: nan\n",
      "Iteration 790 loss: nan\n",
      "Iteration 791 loss: nan\n",
      "Iteration 792 loss: nan\n",
      "Iteration 793 loss: nan\n",
      "Iteration 794 loss: nan\n",
      "Iteration 795 loss: nan\n",
      "Iteration 796 loss: nan\n",
      "Iteration 797 loss: nan\n",
      "Iteration 798 loss: nan\n",
      "Iteration 799 loss: nan\n",
      "Iteration 800 loss: nan\n",
      "Iteration 801 loss: nan\n",
      "Iteration 802 loss: nan\n",
      "Iteration 803 loss: nan\n",
      "Iteration 804 loss: nan\n",
      "Iteration 805 loss: nan\n",
      "Iteration 806 loss: nan\n",
      "Iteration 807 loss: nan\n",
      "Iteration 808 loss: nan\n",
      "Iteration 809 loss: nan\n",
      "Iteration 810 loss: nan\n",
      "Iteration 811 loss: nan\n",
      "Iteration 812 loss: nan\n",
      "Iteration 813 loss: nan\n",
      "Iteration 814 loss: nan\n",
      "Iteration 815 loss: nan\n",
      "Iteration 816 loss: nan\n",
      "Iteration 817 loss: nan\n",
      "Iteration 818 loss: nan\n",
      "Iteration 819 loss: nan\n",
      "Iteration 820 loss: nan\n",
      "Iteration 821 loss: nan\n",
      "Iteration 822 loss: nan\n",
      "Iteration 823 loss: nan\n",
      "Iteration 824 loss: nan\n",
      "Iteration 825 loss: nan\n",
      "Iteration 826 loss: nan\n",
      "Iteration 827 loss: nan\n",
      "Iteration 828 loss: nan\n",
      "Iteration 829 loss: nan\n",
      "Iteration 830 loss: nan\n",
      "Iteration 831 loss: nan\n",
      "Iteration 832 loss: nan\n",
      "Iteration 833 loss: nan\n",
      "Iteration 834 loss: nan\n",
      "Iteration 835 loss: nan\n",
      "Iteration 836 loss: nan\n",
      "Iteration 837 loss: nan\n",
      "Iteration 838 loss: nan\n",
      "Iteration 839 loss: nan\n",
      "Iteration 840 loss: nan\n",
      "Iteration 841 loss: nan\n",
      "Iteration 842 loss: nan\n",
      "Iteration 843 loss: nan\n",
      "Iteration 844 loss: nan\n",
      "Iteration 845 loss: nan\n",
      "Iteration 846 loss: nan\n",
      "Iteration 847 loss: nan\n",
      "Iteration 848 loss: nan\n",
      "Iteration 849 loss: nan\n",
      "Iteration 850 loss: nan\n",
      "Iteration 851 loss: nan\n",
      "Iteration 852 loss: nan\n",
      "Iteration 853 loss: nan\n",
      "Iteration 854 loss: nan\n",
      "Iteration 855 loss: nan\n",
      "Iteration 856 loss: nan\n",
      "Iteration 857 loss: nan\n",
      "Iteration 858 loss: nan\n",
      "Iteration 859 loss: nan\n",
      "Iteration 860 loss: nan\n",
      "Iteration 861 loss: nan\n",
      "Iteration 862 loss: nan\n",
      "Iteration 863 loss: nan\n",
      "Iteration 864 loss: nan\n",
      "Iteration 865 loss: nan\n",
      "Iteration 866 loss: nan\n",
      "Iteration 867 loss: nan\n",
      "Iteration 868 loss: nan\n",
      "Iteration 869 loss: nan\n",
      "Iteration 870 loss: nan\n",
      "Iteration 871 loss: nan\n",
      "Iteration 872 loss: nan\n",
      "Iteration 873 loss: nan\n",
      "Iteration 874 loss: nan\n",
      "Iteration 875 loss: nan\n",
      "Iteration 876 loss: nan\n",
      "Iteration 877 loss: nan\n",
      "Iteration 878 loss: nan\n",
      "Iteration 879 loss: nan\n",
      "Iteration 880 loss: nan\n",
      "Iteration 881 loss: nan\n",
      "Iteration 882 loss: nan\n",
      "Iteration 883 loss: nan\n",
      "Iteration 884 loss: nan\n",
      "Iteration 885 loss: nan\n",
      "Iteration 886 loss: nan\n",
      "Iteration 887 loss: nan\n",
      "Iteration 888 loss: nan\n",
      "Iteration 889 loss: nan\n",
      "Iteration 890 loss: nan\n",
      "Iteration 891 loss: nan\n",
      "Iteration 892 loss: nan\n",
      "Iteration 893 loss: nan\n",
      "Iteration 894 loss: nan\n",
      "Iteration 895 loss: nan\n",
      "Iteration 896 loss: nan\n",
      "Iteration 897 loss: nan\n",
      "Iteration 898 loss: nan\n",
      "Iteration 899 loss: nan\n",
      "Iteration 900 loss: nan\n",
      "Iteration 901 loss: nan\n",
      "Iteration 902 loss: nan\n",
      "Iteration 903 loss: nan\n",
      "Iteration 904 loss: nan\n",
      "Iteration 905 loss: nan\n",
      "Iteration 906 loss: nan\n",
      "Iteration 907 loss: nan\n",
      "Iteration 908 loss: nan\n",
      "Iteration 909 loss: nan\n",
      "Iteration 910 loss: nan\n",
      "Iteration 911 loss: nan\n",
      "Iteration 912 loss: nan\n",
      "Iteration 913 loss: nan\n",
      "Iteration 914 loss: nan\n",
      "Iteration 915 loss: nan\n",
      "Iteration 916 loss: nan\n",
      "Iteration 917 loss: nan\n",
      "Iteration 918 loss: nan\n",
      "Iteration 919 loss: nan\n",
      "Iteration 920 loss: nan\n",
      "Iteration 921 loss: nan\n",
      "Iteration 922 loss: nan\n",
      "Iteration 923 loss: nan\n",
      "Iteration 924 loss: nan\n",
      "Iteration 925 loss: nan\n",
      "Iteration 926 loss: nan\n",
      "Iteration 927 loss: nan\n",
      "Iteration 928 loss: nan\n",
      "Iteration 929 loss: nan\n",
      "Iteration 930 loss: nan\n",
      "Iteration 931 loss: nan\n",
      "Iteration 932 loss: nan\n",
      "Iteration 933 loss: nan\n",
      "Iteration 934 loss: nan\n",
      "Iteration 935 loss: nan\n",
      "Iteration 936 loss: nan\n",
      "Iteration 937 loss: nan\n",
      "Iteration 938 loss: nan\n",
      "Iteration 939 loss: nan\n",
      "Iteration 940 loss: nan\n",
      "Iteration 941 loss: nan\n",
      "Iteration 942 loss: nan\n",
      "Iteration 943 loss: nan\n",
      "Iteration 944 loss: nan\n",
      "Iteration 945 loss: nan\n",
      "Iteration 946 loss: nan\n",
      "Iteration 947 loss: nan\n",
      "Iteration 948 loss: nan\n",
      "Iteration 949 loss: nan\n",
      "Iteration 950 loss: nan\n",
      "Iteration 951 loss: nan\n",
      "Iteration 952 loss: nan\n",
      "Iteration 953 loss: nan\n",
      "Iteration 954 loss: nan\n",
      "Iteration 955 loss: nan\n",
      "Iteration 956 loss: nan\n",
      "Iteration 957 loss: nan\n",
      "Iteration 958 loss: nan\n",
      "Iteration 959 loss: nan\n",
      "Iteration 960 loss: nan\n",
      "Iteration 961 loss: nan\n",
      "Iteration 962 loss: nan\n",
      "Iteration 963 loss: nan\n",
      "Iteration 964 loss: nan\n",
      "Iteration 965 loss: nan\n",
      "Iteration 966 loss: nan\n",
      "Iteration 967 loss: nan\n",
      "Iteration 968 loss: nan\n",
      "Iteration 969 loss: nan\n",
      "Iteration 970 loss: nan\n",
      "Iteration 971 loss: nan\n",
      "Iteration 972 loss: nan\n",
      "Iteration 973 loss: nan\n",
      "Iteration 974 loss: nan\n",
      "Iteration 975 loss: nan\n",
      "Iteration 976 loss: nan\n",
      "Iteration 977 loss: nan\n",
      "Iteration 978 loss: nan\n",
      "Iteration 979 loss: nan\n",
      "Iteration 980 loss: nan\n",
      "Iteration 981 loss: nan\n",
      "Iteration 982 loss: nan\n",
      "Iteration 983 loss: nan\n",
      "Iteration 984 loss: nan\n",
      "Iteration 985 loss: nan\n",
      "Iteration 986 loss: nan\n",
      "Iteration 987 loss: nan\n",
      "Iteration 988 loss: nan\n",
      "Iteration 989 loss: nan\n",
      "Iteration 990 loss: nan\n",
      "Iteration 991 loss: nan\n",
      "Iteration 992 loss: nan\n",
      "Iteration 993 loss: nan\n",
      "Iteration 994 loss: nan\n",
      "Iteration 995 loss: nan\n",
      "Iteration 996 loss: nan\n",
      "Iteration 997 loss: nan\n",
      "Iteration 998 loss: nan\n",
      "Iteration 999 loss: nan\n",
      "Iteration 1000 loss: nan\n"
     ]
    }
   ],
   "source": [
    "w_best , b_best, y_pred = train_via_gradient_descent(X_train, y_train, w, b, learning_rate=0.005, num_iterations=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
